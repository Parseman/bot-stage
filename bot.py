import discord
from discord.ext import commands, tasks
from dotenv import load_dotenv
import os
from datetime import time
import requests
from bs4 import BeautifulSoup
import sqlite3
from keep_alive import keep_alive

# -----------------------------
# CONFIGURATION
# -----------------------------
load_dotenv()
TOKEN = os.getenv("DISCORD_TOKEN")
CHANNEL_ID = 1461140319205851281  # Ton salon ID

# Mots-cl√©s obligatoires dans le titre
KEYWORDS = ["d√©veloppeuse", "d√©veloppeur", "dev", "informatique", "python", "web", "full stack", "backend", "frontend", "data", "software"]

# Header pour simuler un vrai navigateur (Anti-Bot basique)
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36",
    "Accept-Language": "fr-FR,fr;q=0.9,en-US;q=0.8,en;q=0.7"
}

# -----------------------------
# INTENTS
# -----------------------------
intents = discord.Intents.default()
intents.message_content = True
bot = commands.Bot(command_prefix="!", intents=intents)

# -----------------------------
# BASE DE DONN√âES (SQLITE)
# -----------------------------
conn = sqlite3.connect("offres.db")
c = conn.cursor()
c.execute("""
CREATE TABLE IF NOT EXISTS offres (
    lien TEXT PRIMARY KEY
)
""")
conn.commit()

def est_nouvelle(lien):
    """V√©rifie si l'offre est d√©j√† en base, sinon l'ajoute."""
    c.execute("SELECT lien FROM offres WHERE lien=?", (lien,))
    if c.fetchone():
        return False
    c.execute("INSERT INTO offres VALUES (?)", (lien,))
    conn.commit()
    return True

# -----------------------------
# FONCTIONS SCRAPING
# -----------------------------
def get_offres_wtj():
    """Scraping Welcome to the Jungle"""
    print("--- Scraping WTJ ---")
    url = "https://www.welcometothejungle.com/fr/jobs?query=stage%20developpeur&aroundQuery=√éle-de-France"
    offres = []
    
    try:
        r = requests.get(url, headers=HEADERS)
        if r.status_code != 200:
            print(f"‚ùå Erreur WTJ : Status {r.status_code}")
            return []

        soup = BeautifulSoup(r.text, "html.parser")
        
        # Recherche plus large : tous les liens contenant /jobs/ et /companies/
        # Cela √©vite de d√©pendre des classes CSS al√©atoires comme 'sc-j4th9j-0'
        for a in soup.find_all('a', href=True):
            href = a['href']
            if "/fr/companies/" in href and "/fr/jobs/" in href:
                titre = a.text.strip()
                # Filtrage basique pour √©viter les titres vides ou trop longs (bruit)
                if titre and len(titre) < 150:
                    lien = "https://www.welcometothejungle.com" + href
                    # √âviter les doublons dans la m√™me liste
                    if not any(o['lien'] == lien for o in offres):
                        offres.append({"titre": titre, "lien": lien, "source": "Welcome to the Jungle"})
        
        print(f"‚úÖ WTJ : {len(offres)} offres brutes trouv√©es")
        return offres

    except Exception as e:
        print(f"‚ùå Exception WTJ : {e}")
        return []

def get_offres_hellowork():
    """Scraping HelloWork"""
    print("--- Scraping HelloWork ---")
    # L'URL contient d√©j√† les filtres : stage + developpeur + ile-de-france
    url = "https://www.hellowork.com/fr-fr/emploi/recherche.html?k=stage%20developpeur&l=ile-de-france"
    offres = []

    try:
        r = requests.get(url, headers=HEADERS)
        if r.status_code != 200:
            print(f"‚ùå Erreur HelloWork : Status {r.status_code}")
            return []

        soup = BeautifulSoup(r.text, "html.parser")
        
        # HelloWork structure souvent ses titres dans des balises <h3> ou des liens sp√©cifiques
        # On cherche les liens qui ont un attribut title ou une classe li√©e aux offres
        # M√©thode g√©n√©rique : chercher dans la liste des r√©sultats
        
        # S√©lecteur commun HelloWork (peut changer, donc on essaie de viser large sur les liens d'offres)
        items = soup.select("ul.cr-results > li")
        
        if not items:
            # Plan B: chercher n'importe quel lien qui ressemble √† une offre
            items = soup.select("a[href*='/fr-fr/emplois/']")

        for item in items:
            # Si c'est un LI, on cherche le lien A dedans, sinon c'est d√©j√† un A (Plan B)
            tag_a = item.find("a") if item.name == "li" else item
            
            if tag_a and tag_a.get("href"):
                titre = tag_a.text.strip()
                # Nettoyage du titre (parfois HelloWork met le nom de l'entreprise dedans)
                if not titre:
                    continue
                    
                lien = "https://www.hellowork.com" + tag_a.get("href")
                
                # V√©rif doublon liste
                if not any(o['lien'] == lien for o in offres):
                    offres.append({"titre": titre, "lien": lien, "source": "HelloWork"})

        print(f"‚úÖ HelloWork : {len(offres)} offres brutes trouv√©es")
        return offres

    except Exception as e:
        print(f"‚ùå Exception HelloWork : {e}")
        return []

# -----------------------------
# FILTRAGE
# -----------------------------
def filtrer(offres):
    """Filtre les offres par mots-cl√©s uniquement."""
    result = []
    for o in offres:
        titre = o["titre"].lower()

        # 1. V√©rifier mots-cl√©s (Obligatoire)
        if not any(k in titre for k in KEYWORDS):
            continue
        
        # NOTE : On ne filtre plus la LOCALISATION ni la DUR√âE ici.
        # Pourquoi ? Parce que "Stage Python" ne contient pas "√éle-de-France" dans le titre.
        # L'URL de recherche fait d√©j√† le travail de localisation.
        
        result.append(o)
    
    print(f"üìä Apr√®s filtrage mots-cl√©s : {len(result)} offres retenues")
    return result

# -----------------------------
# ENVOI DISCORD
# -----------------------------
async def envoyer_offres_channel(channel):
    offres = []
    
    # R√©cup√©ration
    offres += get_offres_wtj()
    offres += get_offres_hellowork()
    
    # Filtrage
    offres_filtrees = filtrer(offres)

    # V√©rification base de donn√©es (pour ne pas renvoyer les anciennes)
    nouvelles = [o for o in offres_filtrees if est_nouvelle(o["lien"])]
    
    print(f"‚ú® Nouvelles offres √† envoyer : {len(nouvelles)}")

    if not nouvelles:
        await channel.send("Pas de nouvelles offres pour l'instant (mais le script fonctionne !) üïµÔ∏è‚Äç‚ôÇÔ∏è")
        return

    # Envoi
    await channel.send(f"**üì¢ J'ai trouv√© {len(nouvelles)} nouvelle(s) offre(s) !**")
    
    for o in nouvelles:
        embed = discord.Embed(
            title=o["titre"],
            url=o["lien"],
            color=0x00ff00
        )
        embed.set_footer(text=f"Source : {o['source']} | √éle-de-France")
        await channel.send(embed=embed)

# -----------------------------
# T√ÇCHE R√âCURRENTE
# -----------------------------
@tasks.loop(time=time(hour=9, minute=0)) # 9h00 du matin c'est mieux que 1h
async def recherche_quotidienne():
    channel = bot.get_channel(CHANNEL_ID)
    if channel:
        print("‚è∞ Lancement de la t√¢che quotidienne...")
        await envoyer_offres_channel(channel)
    else:
        print(f"‚ùå Channel {CHANNEL_ID} introuvable")

# -----------------------------
# COMMANDES
# -----------------------------
@bot.command()
async def ping(ctx):
    await ctx.send("Pong üèì")

@bot.command()
async def recherche(ctx):
    """Commande manuelle"""
    await ctx.send("üîç Je lance la recherche, patiente un instant...")
    await envoyer_offres_channel(ctx.channel)
    await ctx.send("‚úÖ Recherche termin√©e.")

# -----------------------------
# D√âMARRAGE
# -----------------------------
@bot.event
async def on_ready():
    print(f"ü§ñ Connect√© en tant que {bot.user}")
    print(f"üì¶ Base de donn√©es : connect√©e")
    print(f"üì° Pr√™t √† scraper !")
    if not recherche_quotidienne.is_running():
        recherche_quotidienne.start()

keep_alive()  # <--- Ajoute cette ligne ici
bot.run(TOKEN)